# WSL2 + CUDA 13.0 ç’°å¢ƒæ§‹ç¯‰ã‚¬ã‚¤ãƒ‰ ï½Swallow-13Bã«ã‚ˆã‚‹ç’°å¢ƒæ¤œè¨¼ï½

## ğŸ“œ ã“ã®è¨­è¨ˆå›³ã«ã¤ã„ã¦

ã“ã®è¨­è¨ˆå›³ã¯ã€WSL2 Ubuntu 24.04.1 LTS ç’°å¢ƒã§ã€CUDA 13.0ç’°å¢ƒã‚’æ§‹ç¯‰ã—ã€Swallow-13Bã§å‹•ä½œæ¤œè¨¼ã™ã‚‹ãŸã‚ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

**ç›®çš„:**
- WSL2 + CUDA 13.0ç’°å¢ƒã®å®Œå…¨æ§‹ç¯‰
- Swallow-13Bã«ã‚ˆã‚‹ç’°å¢ƒæ¤œè¨¼ï¼ˆGPUå‹•ä½œç¢ºèªï¼‰
- Mixtral 8x7Bå®Ÿè¡Œã¸ã®æº–å‚™

**ç’°å¢ƒ:**
- OS: Windows 11 + WSL2 Ubuntu 24.04.1 LTS
- GPU: NVIDIA GeForce RTX 5070 Ti 16GB
- CUDA: 13.0
- Python: 3.12 (Miniforge)

## ğŸªŸ Windowså´ã®æº–å‚™ï¼ˆå¿…é ˆï¼‰

### NVIDIA Studioãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®æ›´æ–°

WSL2ã§CUDAã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€Windowså´ã®NVIDIAãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’æœ€æ–°ç‰ˆã«æ›´æ–°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

#### æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹å¼

1. [NVIDIA Drivers ãƒšãƒ¼ã‚¸](https://www.nvidia.com/ja-jp/drivers/)ã«ã‚¢ã‚¯ã‚»ã‚¹
2. è£½å“ã‚¿ã‚¤ãƒ—: GeForce
3. è£½å“ã‚·ãƒªãƒ¼ã‚º: GeForce RTX 50 Series
4. è£½å“: GeForce RTX 5070 Ti
5. ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ : Windows 11
6. ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚¿ã‚¤ãƒ—: Studio Driverï¼ˆæ¨å¥¨ï¼‰
7. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

#### NVIDIAã‚¢ãƒ—ãƒªæ–¹å¼ï¼ˆæ¨å¥¨ï¼‰

GeForce Experienceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‹ã‚‰ã€NVIDIAã‚¢ãƒ—ãƒªï¼ˆNVIDIA Appï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

1. NVIDIAã‚¢ãƒ—ãƒªã‚’èµ·å‹•
2. ã€Œãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã€ã‚¿ãƒ–ã‚’é¸æŠ
3. ã€ŒStudio Driverã€ã‚’é¸æŠï¼ˆAI/MLä½œæ¥­ã«æœ€é©åŒ–ï¼‰
4. ã€Œã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚’é¸æŠ
5. ã€Œã‚¯ãƒªãƒ¼ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã«ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹
6. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œ

**é‡è¦:** ã‚¯ãƒªãƒ¼ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«ã‚ˆã‚Šã€å¤ã„ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®æ®‹éª¸ãŒå®Œå…¨ã«å‰Šé™¤ã•ã‚Œã€æ–°ã—ã„ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒã‚¯ãƒªãƒ¼ãƒ³ãªçŠ¶æ…‹ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¾ã™ã€‚

#### ç¢ºèªæ–¹æ³•

```bash
# WSL2å†…ã§ç¢ºèª
nvidia-smi
# â†’ CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã‚‹
```

## ğŸœï¸ ä»®æƒ³ç ‚æ¼ ã®æº–å‚™ï¼ˆWSL2 Ubuntuï¼‰

### ROPsã®ç¢ºèª

NVIDIAÂ® GeForce RTXâ„¢ 5070 Tiã«ã¯96å€‹ã®Render Output Units (ROPs)ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚
åˆæœŸä¸è‰¯ã®çµŒé¨“ã‹ã‚‰ã€æ°—ã«ãªã‚‹å ´åˆã¯TechPowerUp GPU-Zã§ ROPsã‚’ç¢ºèªã™ã‚‹ã“ã¨ã€‚

### ãƒ“ãƒ«ãƒ‰ç’°å¢ƒã®æº–å‚™

WSL2 Ubuntu 24.04.1 LTSã«ãƒ“ãƒ«ãƒ‰ç’°å¢ƒã‚’æº–å‚™ã™ã‚‹ã€‚

```bash
# ãƒ“ãƒ«ãƒ‰ãƒ„ãƒ¼ãƒ«è¿½åŠ 
sudo apt-get update
sudo apt-get install -y build-essential cmake
```

## âš¡ GPUã®é­”åŠ›ã‚’è§£ãæ”¾ã¤ï¼ˆCUDA 13.0ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰

AIã‚’å‹•ä½œã•ã›ã‚‹ãŸã‚ã€CUDA(Compute Unified Device Architectureï¼šã‚¯ãƒ¼ãƒ€)ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚

```bash
# 1. æ—¢å­˜ã®å¤ã„CUDAé–¢é€£ã‚’å‰Šé™¤
sudo apt-get --purge remove "*cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
  "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" "*nvvm*"
sudo apt-get autoremove
sudo apt-get autoclean

# 2. WSL2ç”¨CUDAãƒªãƒã‚¸ãƒˆãƒªè¿½åŠ 
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update

# 3. CUDA 13.0 Toolkitã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt-get -y install cuda-toolkit-13-0

# 4. ç’°å¢ƒå¤‰æ•°è¨­å®š
echo 'export PATH=/usr/local/cuda-13.0/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-13.0/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# 5. å‹•ä½œç¢ºèª
nvcc --version
```

### ç¢ºèªæ–¹æ³•

```bash
# Step 1: GPUèªè­˜ç¢ºèª
nvidia-smi
```

ã‚µãƒ³ãƒ—ãƒ«ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ä¸‹è¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã§ä½œã‚‹ã€‚

```bash
# Step 2: CUDAã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cat << 'EOF' > test_cuda.cu
#include <stdio.h>

__global__ void hello() {
    printf("Hello from GPU!\n");
}

int main() {
    hello<<<1, 1>>>();
    cudaDeviceSynchronize();
    return 0;
}
EOF
```

ãƒ“ãƒ«ãƒ‰ã—ã€å®Ÿè¡Œã—ã¦ã¿ã‚ˆã†ã€‚`Hello from GPU!`ã¨è¡¨ç¤ºã•ã‚Œã‚Œã°æˆåŠŸã ã€‚

```bash
nvcc test_cuda.cu -o test_cuda
./test_cuda
```

ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’ç¢ºèªã—ã¦ã¿ã‚ˆã†ã€‚

```bash
# Step 3: ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ç¢ºèª
cat << 'EOF' > device_info.cu
#include <stdio.h>

int main() {
    int deviceCount;
    cudaGetDeviceCount(&deviceCount);
    
    printf("CUDA Devices: %d\n", deviceCount);
    
    for(int i = 0; i < deviceCount; i++) {
        cudaDeviceProp prop;
        cudaGetDeviceProperties(&prop, i);
        printf("Device %d: %s\n", i, prop.name);
        printf("  Compute Capability: %d.%d\n", prop.major, prop.minor);
    }
    return 0;
}
EOF
```

ãƒ“ãƒ«ãƒ‰å¾Œã€å®Ÿè¡Œã—ã€`NVIDIA GeForce RTX 5070 Ti`ãŒè¡¨ç¤ºã•ã‚Œã‚Œã°æˆåŠŸã ã€‚

```bash
nvcc device_info.cu -o device_info
./device_info
```

## ğŸ Pythonç’°å¢ƒã®å‰µé€ 

### uvã®å¬å–šï¼ˆé«˜é€Ÿãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ç¥å™¨ï¼‰

pipã®ä»£ã‚ã‚Šã«uvã‚’ä½¿ç”¨ã™ã‚‹ã€‚å…ˆé ­ã«uvã‚’ä»˜ã‘ã‚‹ã ã‘ã€‚

```bash
# uvï¼ˆé«˜é€Ÿ Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Miniforgeç’°å¢ƒã®æº–å‚™ï¼ˆæ¨å¥¨ï¼‰

Miniforgeã¯ã€conda-forgeãƒãƒ£ãƒ³ãƒãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§åˆ©ç”¨ã—ã€Mambaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’å«ã‚€è»½é‡ãªãƒ‡ã‚£ã‚¹ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Anacondaç¤¾ã®å•†ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹åˆ¶é™ã‚’å›é¿ã—ã€é«˜é€Ÿãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```bash
# Miniforge (Mambaforge) ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh
bash Miniforge3-Linux-x86_64.sh
```

ã‚·ã‚§ãƒ«å®Ÿè¡Œå¾Œã®å…¥åŠ›ã¯ä¸‹è¨˜ã®é€šã‚Šã€‚å¾Œã¯ã€Enterã‚­ãƒ¼ã®ã¿ã€‚

```bash
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹åŒæ„
Do you accept the license terms? [yes|no]
>>> yes
```

Miniforgeæœ‰åŠ¹åŒ–

```bash
source ~/miniforge3/bin/activate
```

### Pythonç’°å¢ƒã®ä½œæˆ

å®‰å®šå‹•ä½œã—ãã†ãªã€Python 3.12ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚

```bash
# mamba ã‚’ä½¿ç”¨ã—ã¦ç’°å¢ƒã‚’ä½œæˆï¼ˆMiniforge/Mambaforgeã§ã¯æœ€åˆã‹ã‚‰ mamba ãŒä½¿ãˆã‚‹ï¼‰
mamba create -n ai_env python=3.12 -y
```

ai_envã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã™ã‚‹ã€‚

```bash
eval "$(mamba shell hook --shell bash)"
mamba activate ai_env
```

### AIãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ai_envç’°å¢ƒãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŠ¶æ…‹ã§å®Ÿè¡Œã™ã‚‹ã€‚

#### ã€é‡è¦ã€‘llama-cpp-pythonã¯ã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰å¿…é ˆ

llama-cpp-pythonã¯ã€mambaã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã›ãšã€uvã§å¿…ãšã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ“ãƒ«ãƒ‰ã™ã‚‹ã“ã¨ã€‚

**ç†ç”±:**
- ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ç‰ˆã¯æ±ç”¨çš„ã«ä½œã‚‰ã‚Œã¦ãŠã‚Šã€ç‰¹å®šã®GPUã«æœ€é©åŒ–ã•ã‚Œã¦ã„ãªã„
- ã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ã«ã‚ˆã‚Šã€RTX 5070 Tiï¼ˆCompute Capability 12.0ï¼‰ã«å®Œå…¨æœ€é©åŒ–
- å®Ÿæ¸¬ã§ç´„14å€ã®æ€§èƒ½å·®ï¼ˆè©³ç´°ã¯ã€Œæ€§èƒ½æ¸¬å®šçµæœã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³å‚ç…§ï¼‰

**æ¨å¥¨:** ä»¥ä¸‹ã®æ‰‹é †ã«å¾“ã„ã€å¿…ãšuv(or pip)ã§ã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

#### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †

```bash
# âš ï¸ æ³¨æ„: ai_env ç’°å¢ƒãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŠ¶æ…‹ã§å®Ÿè¡Œ

# âš ï¸ æœ€é‡è¦: NumPy 1.xç³»ã‚’æœ€åˆã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆä¾å­˜é–¢ä¿‚ã‚’å…ˆã«å›ºå®šï¼‰
mamba install -y "numpy>=1.20,<2.0"

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
mamba install -y gxx_linux-64

# ç§‘å­¦è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆNumPy 1.xç’°å¢ƒã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
mamba install -y scipy scikit-learn

# GPUç”¨ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆNumPy 1.xç’°å¢ƒã§ï¼‰
mamba install -y faiss-gpu

# PyTorchã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
mamba install -y pytorch torchvision torchaudio

# âš ï¸ é‡è¦: llama-cpp-python ã¯ uv ã§ CUDA å¯¾å¿œç‰ˆã‚’ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ“ãƒ«ãƒ‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cat > /tmp/constraints.txt << 'EOF'
numpy>=1.20,<2.0
EOF

CMAKE_ARGS="-DGGML_CUDA=on" uv pip install llama-cpp-python --force-reinstall --no-cache-dir --constraint /tmp/constraints.txt

# ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡åŒ–
mamba install -y pyarrow datasets

# æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ–
mamba install -y huggingface_hub

# LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™º
mamba install -y langchain-community langchain-huggingface sentence-transformers

# ãƒ‡ãƒ¢/UIæ§‹ç¯‰
mamba install -y gradio
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€ä¸‹è¨˜ã®ç¢ºèªã‚’è¡Œã†ã€‚

```bash
# æœ€çµ‚ç¢ºèª
python -c "import numpy; print(f'NumPy: {numpy.__version__}')"
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import faiss; print('faiss OK')"
python -c "from llama_cpp import Llama; print('llama-cpp-python OK')"
```

ç¾æ™‚ç‚¹ã®ç‰ˆæ•°ä¾‹ï¼š

```bash
NumPy: 1.26.4
PyTorch: 2.8.0
faiss OK
llama-cpp-python OK
```

## ğŸ¤– ç’°å¢ƒæ¤œè¨¼ï¼ˆSwallow-13Bï¼‰

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

ç’°å¢ƒãŒæ­£ã—ãæ§‹ç¯‰ã•ã‚ŒãŸã‹ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€Swallow-13B Q4_K_Mé‡å­åŒ–ç‰ˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```bash
# Swallow-13B Q4_K_Mé‡å­åŒ–ç‰ˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
wget https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf/resolve/main/tokyotech-llm-Swallow-13b-instruct-v0.1-Q4_K_M.gguf
```

### RAGçŸ¥è­˜ã®æº–å‚™

`rag.txt`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€è³¢è€…ãŒå‚ç…§ã™ã‚‹çŸ¥è­˜ã‚’è¨˜è¼‰ã™ã‚‹ã€‚

### å®Ÿè¡Œæ‰‹é †

```bash
# å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã®èµ·å‹•
python swallow-13b.py
```

èµ·å‹•å¾Œã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨URLãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼š

```
æº–å‚™å®Œäº†ã€‚è³¢è€…ã¨ã®å¯¾è©±ã‚’é–‹å§‹ã§ãã¾ã™ã€‚

Running on local URL:  http://127.0.0.1:7860
```

Ctrlã‚­ãƒ¼ã‚’æŠ¼ã—ãªãŒã‚‰URLã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ãƒ–ãƒ©ã‚¦ã‚¶ã§Gradio UIãŒé–‹ãã¾ã™ã€‚

**ä½¿ç”¨æ–¹æ³•:**
1. ã€Œæ—…äººã‹ã‚‰ã®å•ã„ã‹ã‘ã€æ¬„ã«è³ªå•ã‚’å…¥åŠ›
2. ã€Œè³ªå•ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
3. ç´„1ç§’ã§ã€Œè³¢è€…ã®å¿œç­”ã€æ¬„ã«å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã‚‹

**æ¨è«–æ™‚é–“ã«ã¤ã„ã¦:**
- GPUä½¿ç”¨æ™‚: ç´„1ç§’ï¼ˆå›ç­”ã®æ–‡å­—æ•°ã«ã‚ˆã‚Šå¤‰å‹•ï¼‰
- CPUä½¿ç”¨æ™‚: ç´„20ç§’
- ãƒ—ãƒªãƒ“ãƒ«ãƒ‰GPUç‰ˆ: ç´„10ç§’

ã“ã®ç´„27å€ã®é€Ÿåº¦å·®ãŒã€ã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ç‰ˆã®æœ€å¤§ã®åˆ©ç‚¹ã§ã™ã€‚

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### NumPy 2.xå•é¡Œ

llama-cpp-pythonã‚’pipã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã¨ã€NumPyãŒ2.xã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã•ã‚Œã€faissã¨ã®äº’æ›æ€§å•é¡ŒãŒç™ºç”Ÿã™ã‚‹ã€‚

**è§£æ±ºç­–:** constraints.txtã‚’ä½¿ç”¨ã—ã¦ã€NumPy 1.xç³»ã‚’ç¶­æŒã™ã‚‹ã€‚

```bash
cat > /tmp/constraints.txt << 'EOF'
numpy>=1.20,<2.0
EOF

CMAKE_ARGS="-DGGML_CUDA=on" uv pip install llama-cpp-python --force-reinstall --no-cache-dir --constraint /tmp/constraints.txt
```

### CUDAæœªæ¤œå‡ºå•é¡Œ

llama-cpp-pythonãŒCUDAã‚’èªè­˜ã—ãªã„å ´åˆã€ã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ãŒæ­£ã—ãè¡Œã‚ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

**ç¢ºèªæ–¹æ³•:**

```python
from llama_cpp import Llama
llm = Llama(model_path="tokyotech-llm-Swallow-13b-instruct-v0.1-Q4_K_M.gguf", n_gpu_layers=35)
# GPU layersãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã‹ç¢ºèª
```

### ãƒ¡ãƒ¢ãƒªä¸è¶³å•é¡Œ

13Bãƒ¢ãƒ‡ãƒ«ã¯ç´„8GBä»¥ä¸Šã®VRAMã‚’å¿…è¦ã¨ã™ã‚‹ã€‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã€`n_gpu_layers`ã®å€¤ã‚’æ¸›ã‚‰ã™ã€‚

```python
llm = Llama(model_path="...", n_gpu_layers=20)  # å€¤ã‚’æ¸›ã‚‰ã™
```

## ğŸ“Š æ€§èƒ½æ¸¬å®šçµæœ

RTX 5070 Ti 16GBç’°å¢ƒã§ã®å®Ÿæ¸¬å€¤ï¼ˆSwallow-13B Q4_K_Mé‡å­åŒ–ç‰ˆï¼‰ï¼š

### åŸºæœ¬æ€§èƒ½

- **ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚é–“:** ç´„6ç§’ï¼ˆåˆå›èµ·å‹•æ™‚ã®ã¿ï¼‰
- **GPUä½¿ç”¨ç‡:** ç´„80-90%
- **VRAMä½¿ç”¨é‡:** ç´„8-10GB

### æ¨è«–æ™‚é–“ï¼ˆå›ç­”ã®é•·ã•ã«å¿œã˜ã¦å¤‰å‹•ï¼‰

| å›ç­”ã®é•·ã• | ãƒˆãƒ¼ã‚¯ãƒ³æ•° | æ¨è«–æ™‚é–“ | ç”¨é€”ä¾‹ |
|:---------|:----------|:--------|:------|
| çŸ­æ–‡ | ç´„14-30 tokens | 0.25-0.46ç§’ | ç°¡æ½”ãªå›ç­” |
| ä¸­æ–‡ | ç´„50 tokens | 0.74ç§’ | æ¨™æº–çš„ãªå›ç­” |
| é•·æ–‡ | ç´„107 tokens | 1.39ç§’ | è©³ç´°ãªèª¬æ˜ |

### ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé€Ÿåº¦

- **å¹³å‡é€Ÿåº¦:** ç´„65-77 tokens/second
- **åˆå›å¿œç­”:** ç´„1.67ç§’ï¼ˆ294 tokensã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
- **2å›ç›®ä»¥é™:** ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã«ã‚ˆã‚Šé«˜é€ŸåŒ–

### æœ€é©åŒ–åŠ¹æœ

**prefix-match cache:**
- RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ258 tokensï¼‰ãŒè‡ªå‹•çš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- 2å›ç›®ä»¥é™ã®promptè©•ä¾¡æ™‚é–“ãŒã»ã¼0ç§’ã«çŸ­ç¸®
- é€£ç¶šã—ãŸå•ã„ã‹ã‘ã§å¤§å¹…ãªæ€§èƒ½å‘ä¸Š

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•åˆ¥ã®æ€§èƒ½æ¯”è¼ƒ

| æ–¹å¼ | æ¨è«–æ™‚é–“ï¼ˆ50 tokensï¼‰ | ç›¸å¯¾é€Ÿåº¦ |
|:-----|:--------------------|:--------|
| CPUç‰ˆã®ã¿ | ç´„20ç§’ | 1x |
| mamba installç‰ˆ | ç´„10ç§’ | 2x |
| **ã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ç‰ˆ** | **ç´„0.74ç§’** | **ç´„27x** |

**çµè«–:** ã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ç‰ˆã«ã‚ˆã‚Šã€CPUç‰ˆã¨æ¯”è¼ƒã—ã¦ç´„27å€ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã€‚

## ğŸ¯ æ¬¡å›ãƒ­ã‚°ã‚¤ãƒ³æ™‚ã®æ‰‹é †

æ¬¡å›ãƒ­ã‚°ã‚¤ãƒ³æ™‚ã¯ã€ä¸‹è¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€AIç’°å¢ƒã«å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

```bash
source ~/miniforge3/bin/activate
eval "$(mamba shell hook --shell bash)"
mamba activate ai_env
```

## ğŸ“š å‚è€ƒè³‡æ–™

- [NVIDIA Drivers ãƒšãƒ¼ã‚¸](https://www.nvidia.com/ja-jp/drivers/)
- [CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/)
- [llama-cpp-python GitHub](https://github.com/abetlen/llama-cpp-python)
- [Swallow-13B Model](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
- [LangChain Documentation](https://python.langchain.com/)

## âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …

1. **ã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ã¯å¿…é ˆ:** ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ç‰ˆã¯ç´„14å€é…ã„
2. **NumPy 1.xç³»ç¶­æŒ:** constraints.txtã‚’åˆ©ç”¨ã—ãŸãƒ“ãƒ«ãƒ‰
3. **CUDAç’°å¢ƒå¤‰æ•°:** æ¯å›ãƒ­ã‚°ã‚¤ãƒ³æ™‚ã«è¨­å®šãŒå¿…è¦(AIå°‚ç”¨ãªã‚‰ç’°å¢ƒå¤‰æ•°ã«è¨­å®š)
4. **GPUäº’æ›æ€§:** RTX 50ã‚·ãƒªãƒ¼ã‚ºã¯Compute Capability 12.0ä»¥ä¸ŠãŒå¿…è¦
5. **ç’°å¢ƒæ¤œè¨¼:** Swallow-13Bã§ã®æ¤œè¨¼å¾Œã€Mixtral 8x7Bå®Ÿè¡Œã¸é€²ã‚€

---

**ã“ã®è¨­è¨ˆå›³ã«ã‚ˆã‚Šã€Mixtral 8x7Bå®Ÿè¡Œã®ãŸã‚ã®å®Œç’§ãªç’°å¢ƒãŒå®Œæˆã™ã‚‹ã€‚**